"""
Unified Backend Server for Building Footprint AI Pipeline
Supports both Development/Testing and Production deployment

This single backend handles:
- Development testing with minimal dependencies
- Production deployment with full features
- ML pipeline integration with main.py
- API endpoints for both internal and external use
"""

import os
import sys
import logging
from pathlib import Path
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse, FileResponse
from contextlib import asynccontextmanager
from typing import Optional, List, Dict, Any
import uvicorn
import asyncio

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# Import ML pipeline components
ML_PIPELINE_AVAILABLE = False
try:
    from src.config import Config
    ML_PIPELINE_AVAILABLE = True
    logging.info("ML Pipeline config available")
    
    # Only import pipeline when needed, not at startup
    BuildingFootprintPipeline = None
except Exception as e:
    ML_PIPELINE_AVAILABLE = False
    logging.warning(f"ML Pipeline not available - running in API-only mode: {e}")

# Import production components (optional)
try:
    from app.core.config import get_settings
    from app.api.v1.endpoints import auth, buildings, ml_processing
    PRODUCTION_FEATURES = True
except ImportError:
    PRODUCTION_FEATURES = False
    logging.info("Running in development mode - production features disabled")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Environment detection
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")
IS_PRODUCTION = ENVIRONMENT == "production"
IS_DEVELOPMENT = ENVIRONMENT == "development"

# Global ML pipeline instance
ml_pipeline: Optional[BuildingFootprintPipeline] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan - startup and shutdown"""
    global ml_pipeline
    
    # Startup
    logger.info(f"ðŸš€ Starting Building Footprint AI Backend ({ENVIRONMENT} mode)")
    
    # Initialize ML pipeline if available (lazy loading)
    if ML_PIPELINE_AVAILABLE:
        try:
            cfg = Config()
            # Don't initialize pipeline at startup to avoid import errors
            # ml_pipeline = BuildingFootprintPipeline(cfg)
            logger.info("âœ… ML Pipeline config ready (will load on demand)")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize ML pipeline config: {e}")
            ml_pipeline = None
    
    yield
    
    # Shutdown
    logger.info("ðŸ›‘ Shutting down Building Footprint AI Backend")

# Create FastAPI application
app = FastAPI(
    title="Building Footprint AI - Unified Backend",
    description="Production-ready backend for building footprint extraction with ML pipeline integration",
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs" if not IS_PRODUCTION else "/api/docs",
    redoc_url="/redoc" if not IS_PRODUCTION else "/api/redoc"
)

# Add middleware
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"] if IS_DEVELOPMENT else ["https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Health and system endpoints
@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Building Footprint AI Backend",
        "environment": ENVIRONMENT,
        "version": "1.0.0",
        "ml_pipeline_available": ML_PIPELINE_AVAILABLE,
        "production_features": PRODUCTION_FEATURES
    }

@app.get("/health")
async def health_check():
    """Comprehensive health check"""
    health_status = {
        "status": "healthy",
        "environment": ENVIRONMENT,
        "timestamp": "2025-09-26T00:00:00Z",
        "services": {}
    }
    
    # Check geospatial libraries
    try:
        from osgeo import gdal
        import rasterio
        import geopandas as gpd
        health_status["services"]["geospatial"] = {
            "status": "healthy",
            "gdal_version": gdal.__version__,
            "rasterio_version": rasterio.__version__,
            "geopandas_version": gpd.__version__
        }
    except ImportError as e:
        health_status["services"]["geospatial"] = {
            "status": "error",
            "message": f"Missing geospatial libraries: {e}"
        }
    
    # Check ML pipeline
    if ML_PIPELINE_AVAILABLE and ml_pipeline:
        health_status["services"]["ml_pipeline"] = {
            "status": "healthy",
            "message": "ML pipeline initialized"
        }
    else:
        health_status["services"]["ml_pipeline"] = {
            "status": "unavailable",
            "message": "ML pipeline not initialized"
        }
    
    # Check database (production only)
    if PRODUCTION_FEATURES:
        try:
            # Add database health check here
            health_status["services"]["database"] = {
                "status": "healthy",
                "message": "Database connected"
            }
        except Exception as e:
            health_status["services"]["database"] = {
                "status": "error",
                "message": str(e)
            }
    
    return health_status

# ML Processing Endpoints
@app.get("/api/v1/capabilities")
async def get_capabilities():
    """Get system capabilities"""
    capabilities = {
        "ml_processing": ML_PIPELINE_AVAILABLE,
        "production_features": PRODUCTION_FEATURES,
        "supported_formats": ["tif", "tiff", "jpg", "jpeg", "png"],
        "max_file_size_mb": 100,
        "processing_modes": []
    }
    
    if ML_PIPELINE_AVAILABLE:
        capabilities["processing_modes"].extend([
            "single_image",
            "batch_processing", 
            "state_processing",
            "experimental_mode"
        ])
    
    return capabilities

@app.post("/api/v1/process-image")
async def process_single_image(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    mode: str = "single",
    regularize: bool = True
):
    """Process a single image for building extraction"""
    if not ML_PIPELINE_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="ML pipeline not available"
        )
    
    # Validate file
    if not file.filename.lower().endswith(('.tif', '.tiff', '.jpg', '.jpeg', '.png')):
        raise HTTPException(
            status_code=400,
            detail="Unsupported file format"
        )
    
    try:
        # Save uploaded file temporarily
        upload_dir = Path("temp_uploads")
        upload_dir.mkdir(exist_ok=True)
        file_path = upload_dir / file.filename
        
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        # Process with ML pipeline
        result = await process_image_with_pipeline(str(file_path), mode, regularize)
        
        # Clean up temporary file
        file_path.unlink()
        
        return {
            "status": "completed",
            "filename": file.filename,
            "processing_mode": mode,
            "regularization_applied": regularize,
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Processing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/process-state")
async def process_state_data(
    state_name: str,
    background_tasks: BackgroundTasks,
    max_patches: int = 10
):
    """Process state-level building footprint data"""
    if not ML_PIPELINE_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="ML pipeline not available"
        )
    
    # Add to background task
    task_id = f"state_{state_name}_{hash(state_name) % 10000}"
    background_tasks.add_task(
        process_state_background,
        state_name,
        max_patches,
        task_id
    )
    
    return {
        "status": "accepted",
        "task_id": task_id,
        "state": state_name,
        "max_patches": max_patches,
        "message": f"State processing started for {state_name}"
    }

@app.get("/api/v1/demo")
async def run_demo_pipeline(
    samples: int = 5,
    mode: str = "synthetic"
):
    """Run a demonstration of the ML pipeline"""
    if not ML_PIPELINE_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="ML pipeline not available"
        )
    
    try:
        # Run demo through main.py logic
        demo_result = await run_pipeline_demo(samples, mode)
        return {
            "status": "completed",
            "demo_type": mode,
            "samples_processed": samples,
            "results": demo_result
        }
    except Exception as e:
        logger.error(f"Demo failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Development/Testing Endpoints
@app.get("/test/gdal")
async def test_gdal():
    """Test GDAL functionality"""
    try:
        from osgeo import gdal
        
        driver_count = gdal.GetDriverCount()
        drivers = []
        
        for i in range(min(5, driver_count)):
            driver = gdal.GetDriver(i)
            drivers.append({
                "name": driver.ShortName,
                "description": driver.LongName
            })
            
        return {
            "status": "success",
            "gdal_version": gdal.__version__,
            "driver_count": driver_count,
            "sample_drivers": drivers
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/test/process-sample")
async def process_sample():
    """Demo endpoint for basic geospatial processing"""
    try:
        import numpy as np
        import shapely.geometry as sg
        
        # Create sample data
        sample_raster = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
        
        # Sample building footprint
        sample_footprint = sg.Polygon([
            (10, 10), (10, 40), (40, 40), (40, 10), (10, 10)
        ])
        
        # Basic operations
        area = sample_footprint.area
        perimeter = sample_footprint.length
        centroid = sample_footprint.centroid
        
        return {
            "status": "processing_complete",
            "input": {
                "raster_shape": sample_raster.shape,
                "raster_type": str(sample_raster.dtype)
            },
            "extracted_footprint": {
                "area": area,
                "perimeter": perimeter,
                "centroid": [centroid.x, centroid.y],
                "vertices_count": len(sample_footprint.exterior.coords)
            },
            "processing_steps": [
                "âœ… Generated sample raster data",
                "âœ… Created building footprint geometry",
                "âœ… Computed geometric properties",
                "âœ… Validated spatial operations"
            ]
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

# Helper functions for ML processing
async def process_image_with_pipeline(file_path: str, mode: str, regularize: bool) -> Dict[str, Any]:
    """Process image using ML pipeline"""
    if not ml_pipeline:
        raise Exception("ML pipeline not initialized")
    
    try:
        # This would integrate with your actual pipeline
        # For now, return a mock result
        return {
            "buildings_detected": 5,
            "total_area": 1250.0,
            "confidence_score": 0.87,
            "processing_time_seconds": 2.5,
            "regularization_applied": regularize
        }
    except Exception as e:
        logger.error(f"Pipeline processing failed: {e}")
        raise

async def process_state_background(state_name: str, max_patches: int, task_id: str):
    """Background task for state processing"""
    logger.info(f"Starting state processing: {state_name} (Task: {task_id})")
    
    try:
        # This would integrate with your state processing logic
        # Simulate processing
        await asyncio.sleep(1)  # Placeholder
        
        logger.info(f"Completed state processing: {state_name}")
        
    except Exception as e:
        logger.error(f"State processing failed for {state_name}: {e}")

async def run_pipeline_demo(samples: int, mode: str) -> Dict[str, Any]:
    """Run ML pipeline demo"""
    if not ml_pipeline:
        raise Exception("ML pipeline not initialized")
    
    # This would call your main.py demo logic
    return {
        "demo_completed": True,
        "samples_generated": samples,
        "mode": mode,
        "metrics": {
            "average_accuracy": 0.92,
            "processing_speed": "1.2 images/second"
        }
    }

# Production mode includes additional endpoints
if PRODUCTION_FEATURES:
    # Import and include production routers
    try:
        app.include_router(auth.router, prefix="/api/v1/auth", tags=["Authentication"])
        app.include_router(buildings.router, prefix="/api/v1/buildings", tags=["Buildings"])
        app.include_router(ml_processing.router, prefix="/api/v1/ml", tags=["ML Processing"])
    except Exception as e:
        logger.warning(f"Could not load production routers: {e}")

# Run server
if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "127.0.0.1")
    
    logger.info(f"Starting server on {host}:{port}")
    
    uvicorn.run(
        "unified_backend:app",
        host=host,
        port=port,
        reload=IS_DEVELOPMENT,
        workers=1 if IS_DEVELOPMENT else 4,
        access_log=True
    )